---
title: "Chapter4"
author: "Justin Box"
date: "11/25/2018"
output: html_document
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(MASS)
library(corrplot)
data("Boston")
boston_scaled <- scale(Boston)
```

#IODS Week 4

*Clustering and Classification*

##About the Data

This week, I investigated the Boston dataset which contains data of housing values in the suburbs of Boston. Boston dataset structure and dimensions are:

```{r, include=TRUE}
str(Boston)

dim(Boston)
```

All variables are further explained [HERE](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

##Graphing the Data

```{r, include=TRUE}
pairs(Boston, lower.panel = NULL)
```

This representation is difficult to interpret, so next, I will use a correlation matrix to more easily visualize the data. 

```{r, include=TRUE}
cor_matrix<-cor(Boston) %>% round(digits=2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

Here, it is much easier to see the correlations. According to this matrix *rad* (index of accessibility to radial highways) and *tax* (full-value property-tax rate per $10,000) have the highest positive correlation. 

##Standardizing the Data

After scaling, the summary of the data is now available:
```{r, include=TRUE}
summary(boston_scaled)
```

Now the data is normalized and mean centerd. 

Next, I created a categorical variable *crime* of the crime rate (*crim*) by using its quantiles as break points. Then, I removed the old crime rate variable (*crim*) from the dataset and added the new variable *crime*. This was done with the following R code:

```{r, include=TRUE}
boston_scaled <- as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
``` 

The final step before performing the Linear Discriminant Analysis (LDA) is to separate the dataset into training (*train*) and testing (*test*) sets with a ratio of 4:1.

```{r, include=TRUE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

##Linear Discriminant Analysis

Next, I used the LDA on the training set *train*. This involves using the categorical crime rate *crime* as the target variable and all other variables as predictor variables.

```{r, include=TRUE}
lda.fit <- lda(crime ~ ., data = train)
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
```

Here you can see that the *high* crime rate grouped more closely than the others. 

##Making Predictions

First, I saved the crime categories from the test set and then removed the categorical crime variable from the test dataset. 

```{r, include=TRUE}
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
```

Now I can cross tabulate the results with the crime categories from the test set.

```{r, include=TRUE}
table(correct = correct_classes, predicted = lda.pred$class)
```

From this table, we can see that the predicitons are fairly accurate depending on the category. In this model, the *high* was correctly predicted 25 times and incorrectly only 1. *Med-high*, however, was correctly predicted 14 times and incorrectly 5 times (4 in med_low and 1 in low). 

##CLusters/Distance Measures 

First, the dataset "Boston" has to be re-loaded and re-scaled. 
```{r, include=TRUE}
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```


Using the euclidean and manhattan distances, the distance between observations can be measured. Then, a k-means algorithm can be run on the dataset. 

```{r, include=TRUE}
km <-kmeans(boston_scaled, centers = 5)
summary(km)
```

Finally, I can investigate the optimal number of clusters then run the algorithm again. 

```{r, include=TRUE}
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

It seems that the WCSS decreases significantly between 2-3, so I tried with just 2 clusters. 

```{r, include=TRUE}
km <-kmeans(boston_scaled, centers = 2)
pairs(boston_scaled[3:7], col = km$cluster, lower.panel = NULL)
km <-kmeans(boston_scaled, centers = 3)
pairs(boston_scaled[3:7], col = km$cluster, lower.panel = NULL)
```

With just visual inspection, it appears that 2 clusters appears to be the best option. 
